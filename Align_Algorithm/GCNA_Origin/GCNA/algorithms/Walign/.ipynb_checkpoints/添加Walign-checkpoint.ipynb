{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d49e4e4a-46e3-45ad-aa12-025383b75ffc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from abc import ABCMeta\n",
    "\n",
    "class NetworkAlignmentModel:\n",
    "    __metaclass__ = ABCMeta\n",
    "\n",
    "    def __init__(self, source_dataset, target_dataset):\n",
    "        '''Initialize the Embedding class\n",
    "\n",
    "        Args:\n",
    "            source_dataset: source dataset for the alignment\n",
    "            target_dataset: target dataset for the alignment\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "    def align(self):\n",
    "        '''Align the source and target dataset, generate an alignment matrix. '''\n",
    "\n",
    "        pass\n",
    "\n",
    "    def get_alignment_matrix(self):\n",
    "        ''' Returns the generated alignment matrix\n",
    "        Return:\n",
    "            A numpy array of size #nodes * d\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "    def get_source_embedding(self):\n",
    "        ''' Returns the learnt embedding of source dataset (if the method generate the embedding)\n",
    "\n",
    "        Return:\n",
    "            A numpy array of size #nodes * d\n",
    "        '''\n",
    "        return None\n",
    "\n",
    "    def get_target_embedding(self):\n",
    "        ''' Returns the learnt embedding of target dataset (if the method generate the embedding)\n",
    "\n",
    "        Return:\n",
    "            A numpy array of size #nodes * d\n",
    "        '''\n",
    "        return None\n",
    "import json\n",
    "import os\n",
    "import argparse\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from networkx.readwrite import json_graph\n",
    "\n",
    "class Dataset:\n",
    "    \"\"\"\n",
    "    this class receives input from graphsage format with predefined folder structure, the data folder must contains these files:\n",
    "    G.json, id2idx.json, features.npy (optional)\n",
    "\n",
    "    Arguments:\n",
    "    - data_dir: Data directory which contains files mentioned above.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self._load_id2idx()\n",
    "        self._load_G() #用nx来创建图，并获取里面的节点数和边数\n",
    "        self._load_features()\n",
    "        construct_adjacency(self.G, self.id2idx, sparse=False, file_path=self.data_dir + \"/edges.edgelist\")\n",
    "        # self.load_edge_features()\n",
    "        print(\"Dataset info:\")\n",
    "        print(\"- Nodes: \", len(self.G.nodes()))\n",
    "        print(\"- Edges: \", len(self.G.edges()))\n",
    "\n",
    "    def _load_G(self):\n",
    "        G_data = json.load(open(os.path.join(self.data_dir, \"G.json\")))\n",
    "        \" 相当于把G里面的links后面的又改动了，即是真实的图节点了 \"\n",
    "        G_data['links'] = [{'source': self.idx2id[G_data['links'][i]['source']], 'target': self.idx2id[G_data['links'][i]['target']]} for i in range(len(G_data['links']))]\n",
    "        self.G = json_graph.node_link_graph(G_data) #构建图\n",
    "\n",
    "\n",
    "    def _load_id2idx(self):\n",
    "        id2idx_file = os.path.join(self.data_dir, 'id2idx.json')\n",
    "        self.id2idx = json.load(open(id2idx_file))\n",
    "        self.idx2id = {v:k for k,v in self.id2idx.items()}\n",
    "\n",
    "\n",
    "    def _load_features(self):\n",
    "        self.features = None\n",
    "        feats_path = os.path.join(self.data_dir, 'feats.npy')\n",
    "        if os.path.isfile(feats_path):\n",
    "            self.features = np.load(feats_path)\n",
    "        else:\n",
    "            self.features = None\n",
    "        return self.features\n",
    "\n",
    "    def load_edge_features(self):\n",
    "        self.edge_features= None\n",
    "        feats_path = os.path.join(self.data_dir, 'edge_feats.mat')\n",
    "        if os.path.isfile(feats_path):\n",
    "            edge_feats = loadmat(feats_path)['edge_feats']\n",
    "            self.edge_features = np.zeros((len(edge_feats[0]),\n",
    "                                           len(self.G.nodes()),\n",
    "                                           len(self.G.nodes())))\n",
    "            for idx, matrix in enumerate(edge_feats[0]):\n",
    "                self.edge_features[idx] = matrix.toarray()\n",
    "        else:\n",
    "            self.edge_features = None\n",
    "        return self.edge_features\n",
    "\n",
    "    \"\"\"\n",
    "        构建邻接矩阵\n",
    "    \"\"\"\n",
    "    def get_adjacency_matrix(self, sparse=False):\n",
    "        return construct_adjacency(self.G, self.id2idx, sparse=False, file_path=self.data_dir + \"/edges.edgelist\")\n",
    "\n",
    "    def get_nodes_degrees(self):\n",
    "        return build_degrees(self.G, self.id2idx)\n",
    "\n",
    "    def get_nodes_clustering(self):\n",
    "        return build_clustering(self.G, self.id2idx)\n",
    "\n",
    "    def get_edges(self):\n",
    "        return get_edges(self.G, self.id2idx)\n",
    "\n",
    "    def check_id2idx(self):\n",
    "        # print(\"Checking format of dataset\")\n",
    "        for i, node in enumerate(self.G.nodes()):\n",
    "            if (self.id2idx[node] != i):\n",
    "                print(\"Failed at node %s\" % str(node))\n",
    "                return False\n",
    "        # print(\"Pass\")\n",
    "        return True\n",
    "def print_graph_stats(G):\n",
    "    print('# of nodes: %d, # of edges: %d' % (G.number_of_nodes(),\n",
    "                                              G.number_of_edges()))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    构建邻接矩阵，传过来的一共有俩参数，一个是G（包括图的节点啊等等），另外一个是Id2idx\n",
    "\"\"\"\n",
    "def construct_adjacency(G, id2idx, sparse=False, file_path=None):\n",
    "    idx2id = {v:k for k,v in id2idx.items()} #将idx2idx倒过来，本来是'728':1,现在是1:'728'\n",
    "    nodes_list = [idx2id[i] for i in range(len(id2idx))] #将节点全部取出来，构成了nodes列表，即V\n",
    "    edges_list = list(G.edges()) #取G中的边，构成边list，即E\n",
    "\n",
    "    \"\"\"\n",
    "        构建边\n",
    "            edge：{tuple:2}\n",
    "        最后的形式是N * 2的形式，即我们看到的：\n",
    "            0,1\n",
    "            0,2\n",
    "            1,1272\n",
    "            1,451\n",
    "            1,1357\n",
    "            1,1880\n",
    "            2,243\n",
    "            2,241\n",
    "            2,896\n",
    "            2,932\n",
    "            2,2327\n",
    "            2,561\n",
    "            ...\n",
    "    \"\"\"\n",
    "    edges = np.array([[id2idx[edge[0]], id2idx[edge[1]]] for edge in edges_list])\n",
    "    \"\"\"\n",
    "        存储下来\n",
    "    \"\"\"\n",
    "    if file_path:\n",
    "        np.save(file_path, edges)\n",
    "\n",
    "    \"\"\"\n",
    "        如果是sparse=true，以稀疏矩阵的形式存储，并返回的是scipy\n",
    "        如果是sparse=false，将图转换为矩阵,矩阵的数值为边的权重;\n",
    "                    也可以用nx.to_numpy_array(G)，二者等价\n",
    "    \"\"\"\n",
    "    if sparse:\n",
    "        adj = nx.to_scipy_sparse_matrix(G, nodes_list).tolil() #以稀疏矩阵来代替\n",
    "    else:\n",
    "        adj = nx.to_numpy_matrix(G, nodes_list) #不以稀疏矩阵来代替\n",
    "    \"\"\"\n",
    "        此文件是构建邻接矩阵\n",
    "    \"\"\"\n",
    "    return adj\n",
    "\n",
    "\n",
    "def build_degrees(G, id2idx):\n",
    "    degrees = np.zeros(len(G.nodes()))\n",
    "    for node in G.nodes():\n",
    "        deg = G.degree(node)\n",
    "        degrees[id2idx[node]] = deg\n",
    "    return degrees\n",
    "\n",
    "\n",
    "def build_clustering(G, id2idx):\n",
    "    cluster = nx.clustering(G)\n",
    "    # convert clustering from dict with keys are ids to array index-based\n",
    "    clustering = [0] * len(G.nodes())\n",
    "    for id, val in cluster.items():\n",
    "        clustering[id2idx[id]] = val\n",
    "    return clustering\n",
    "\n",
    "\n",
    "def get_H(path, source_dataset, target_dataset, train_dict=\"\"):\n",
    "    if train_dict is not None:\n",
    "        H = np.zeros((len(target_dataset.G.nodes()), len(source_dataset.G.nodes()))) \n",
    "        for k, v in train_dict.items():\n",
    "            H[v, k] = 0.98\n",
    "        return H\n",
    "    if path is None: \n",
    "        H = np.ones((len(target_dataset.G.nodes()), len(source_dataset.G.nodes())))\n",
    "        H = H*(1/len(source_dataset.G.nodes()))\n",
    "        return H\n",
    "    else:    \n",
    "        if not os.path.exists(path):\n",
    "            raise Exception(\"Path '{}' is not exist\".format(path))\n",
    "        dict_H = loadmat(path)\n",
    "        H = dict_H['H']\n",
    "        return H\n",
    "\n",
    "\n",
    "def get_edges(G, id2idx):\n",
    "    edges1 = [(id2idx[n1], id2idx[n2]) for n1, n2 in G.edges()]\n",
    "    edges2 = [(id2idx[n2], id2idx[n1]) for n1, n2 in G.edges()]\n",
    "    \n",
    "    edges = edges1 + edges2\n",
    "    edges = np.array(edges)\n",
    "    return edges\n",
    "\n",
    "\n",
    "def load_gt(path, id2idx_src=None, id2idx_trg=None, format='matrix'):    \n",
    "    if id2idx_src:\n",
    "        \"\"\"\n",
    "            id2idx_src代表的是源id索引号，一共有3906个节点\n",
    "            id2idx_trg代表的是目标id索引号，一共有1118个节点\n",
    "        \"\"\"\n",
    "        conversion_src = type(list(id2idx_src.keys())[0]) #str\n",
    "        conversion_trg = type(list(id2idx_trg.keys())[0]) #str\n",
    "    if format == 'matrix':\n",
    "        \"\"\"\n",
    "            如果是矩阵形式，则代表是SpareTensor的形式，需要给其进行处理\n",
    "        \"\"\"\n",
    "        # Dense\n",
    "        \"\"\"\n",
    "        gt = np.zeros((len(id2idx_src.keys()), len(id2idx_trg.keys())))\n",
    "        with open(path) as file:\n",
    "            for line in file:\n",
    "                src, trg = line.strip().split()                \n",
    "                gt[id2idx_src[conversion_src(src)], id2idx_trg[conversion_trg(trg)]] = 1\n",
    "        return gt\n",
    "        \"\"\"\n",
    "        # Sparse\n",
    "        row = []\n",
    "        col = []\n",
    "        val = []\n",
    "        with open(path) as file:\n",
    "            for line in file:\n",
    "                src, trg = line.strip().split()\n",
    "                row.append(id2idx_src[conversion_src(src)])\n",
    "                col.append(id2idx_trg[conversion_trg(trg)])\n",
    "                val.append(1)\n",
    "        gt = csr_matrix((val, (row, col)), shape=(len(id2idx_src), len(id2idx_trg)))\n",
    "    else:\n",
    "        gt = {} #真实Ground_truth的一个列表\n",
    "        with open(path) as file: #打开文件\n",
    "            for line in file: #按行读，并处理\n",
    "                src, trg = line.strip().split()\n",
    "                # print(src, trg)\n",
    "                if id2idx_src:\n",
    "                    gt[id2idx_src[conversion_src(src)]] = id2idx_trg[conversion_trg(trg)] #将对应的索引号进行对应\n",
    "                else:\n",
    "                    gt[str(src)] = str(trg)\n",
    "    \"\"\"\n",
    "        ground_truth存储的是两个图当中的节点索引号\n",
    "    \"\"\"\n",
    "    return gt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f8cd741-9ba4-4fa2-8412-9978c63a9413",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#class CombUnweighted():\n",
    "    \n",
    "class Walign(NetworkAlignmentModel):\n",
    "    def __init__(self,source_dataset,target_dataset,args,**params):\n",
    "        super(Walign, self).__init__(source_dataset,target_dataset)\n",
    "        self.source_dataset = source_dataset\n",
    "        self.target_dataset = target_dataset\n",
    "        #self.alphas = params.alphas\n",
    "        #self.full_dict = load_gt()\n",
    "    \n",
    "    def align(self):\n",
    "        #source_A_hat, target_A_hat, source_feats, target_feats = self.\n",
    "        networks = []\n",
    "        num_graph = 2\n",
    "        model = 2\n",
    "        \n",
    "    \n",
    "    \n",
    "    def get_elements(self):\n",
    "        \"\"\"\n",
    "        Compute Normalized Laplacian matrix\n",
    "        Preprocessing nodes attribute\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "            构建source和target的A_hat。\n",
    "                首先调用source_dataset.get_adjacency_matrix()构建邻接矩阵\n",
    "                然后再构建拉普拉斯矩阵\n",
    "        \"\"\"\n",
    "        source_A_hat, _ = Laplacian_graph(self.source_dataset.get_adjacency_matrix())\n",
    "        target_A_hat, _ = Laplacian_graph(self.target_dataset.get_adjacency_matrix())\n",
    "        \"\"\"\n",
    "            如果是GPU\n",
    "        \"\"\"\n",
    "        source_A_hat = source_A_hat.cuda()\n",
    "        target_A_hat = target_A_hat.cuda()\n",
    "\n",
    "        \"\"\"\n",
    "            属性特征\n",
    "        \"\"\"\n",
    "        source_feats = self.source_dataset.features #大小为3906 * 538\n",
    "        target_feats = self.target_dataset.features #大小为1118 * 538\n",
    "\n",
    "        if source_feats is None:\n",
    "            source_feats = np.zeros((len(self.source_dataset.G.nodes()), 1))\n",
    "            target_feats = np.zeros((len(self.target_dataset.G.nodes()), 1))\n",
    "\n",
    "        \"\"\"\n",
    "            底下这个是什么操作？！\n",
    "        \"\"\"\n",
    "        for i in range(len(source_feats)): #3906\n",
    "            if source_feats[i].sum() == 0:\n",
    "                source_feats[i, -1] = 1\n",
    "        for i in range(len(target_feats)):\n",
    "            if target_feats[i].sum() == 0:\n",
    "                target_feats[i, -1] = 1\n",
    "\n",
    "\n",
    "        if source_feats is not None:\n",
    "            source_feats = torch.FloatTensor(source_feats) #转成float\n",
    "            target_feats = torch.FloatTensor(target_feats)\n",
    "            source_feats = source_feats.cuda()\n",
    "            target_feats = target_feats.cuda()\n",
    "        \"\"\"\n",
    "        torch.nn.functional.normalize(input, p=2, dim=1, eps=1e-12, out=None)\n",
    "            对指定维度进行L2_norm的计算，即P范数计算（不指定默认为2），对输入数据进行标准化使得输入数据满足正态分布\n",
    "        \"\"\"\n",
    "        source_feats = F.normalize(source_feats)\n",
    "        target_feats = F.normalize(target_feats)\n",
    "        return source_A_hat, target_A_hat, source_feats, target_feats\n",
    "    \n",
    "    def get_adjacency_matrix(self, sparse=False):\n",
    "        return construct_adjacency(self.G, self.id2idx, sparse=False, file_path=self.data_dir + \"/edges.edgelist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d66f2c7-0685-4192-9099-d228783088c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset info:\n",
      "- Nodes:  3906\n",
      "- Edges:  8164\n",
      "Dataset info:\n",
      "- Nodes:  1118\n",
      "- Edges:  1511\n"
     ]
    }
   ],
   "source": [
    "source_dataset = Dataset(\"../../graph_data/douban/online/graphsage\")\n",
    "target_dataset = Dataset(\"../../graph_data/douban/offline/graphsage/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc3ba841-6e1c-44b7-bbbc-3347dccfc4e2",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'graph_data/douban/dictionaries/groundtruth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_51863/2675389195.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgroundtruth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_gt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"graph_data/douban/dictionaries/groundtruth\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msource_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'dict'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroundtruth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcoo_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_51863/1228162842.py\u001b[0m in \u001b[0;36mload_gt\u001b[0;34m(path, id2idx_src, id2idx_trg, format)\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0mgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;31m#真实Ground_truth的一个列表\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#打开文件\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#按行读，并处理\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                 \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'graph_data/douban/dictionaries/groundtruth'"
     ]
    }
   ],
   "source": [
    "groundtruth = load_gt(\"graph_data/douban/dictionaries/groundtruth\",source_dataset.id2idx,target_dataset.id2idx,'dict')\n",
    "print(len(groundtruth))\n",
    "from scipy.sparse import coo_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d0e25fec-dd64-48f8-8ceb-0210f75f3142",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(indices=tensor([[   0,    0,    0,  ..., 3904, 3905, 3905],\n",
       "                        [   0,    1,    2,  ..., 3904,   72, 3905]]),\n",
       "        values=tensor([0.3333, 0.2357, 0.1021,  ..., 0.5000, 0.2357, 0.5000]),\n",
       "        device='cuda:0', size=(3906, 3906), nnz=20234, layout=torch.sparse_coo),\n",
       " tensor(indices=tensor([[   0,    0,    1,  ..., 1116, 1117, 1117],\n",
       "                        [   0,    1,    0,  ..., 1116,  105, 1117]]),\n",
       "        values=tensor([0.5000, 0.3536, 0.3536,  ..., 0.5000, 0.1715, 0.5000]),\n",
       "        device='cuda:0', size=(1118, 1118), nnz=4140, layout=torch.sparse_coo),\n",
       " tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "w = Walign(source_dataset,target_dataset,1)\n",
    "w.get_elements()\n",
    "a,b,c,d = w.get_elements()\n",
    "a.data,b.data,c.data\n",
    "\n",
    "e1 = construct_adjacency(source_dataset.G,source_dataset.id2idx)\n",
    "e2 = construct_adjacency(target_dataset.G,target_dataset.id2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "00b4c527-6c36-4110-a8d6-0dbb3df9bb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric\n",
    "from torch_geometric.nn.conv import MessagePassing, GCNConv\n",
    "class CombUnweighted(MessagePassing):\n",
    "    def __init__(self, K=1, cached=False, bias=True,\n",
    "                 **kwargs):\n",
    "        super(CombUnweighted, self).__init__(aggr='add', **kwargs)\n",
    "        self.K = K\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        edge_index, norm = GCNConv.norm(edge_index, x.size(0), edge_weight,\n",
    "                                        dtype=x.dtype)\n",
    "        xs = [x]\n",
    "        for k in range(self.K):\n",
    "            xs.append(self.propagate(edge_index, x=xs[-1], norm=norm))\n",
    "        return torch.cat(xs, dim = 1)\n",
    "        # return torch.stack(xs, dim=0).mean(dim=0)\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        return norm.view(-1, 1) * x_j\n",
    "\n",
    "    # 打印形式是： CombUnweighted(in_channels, out_channels, K=8)\n",
    "    def __repr__(self):\n",
    "        return '{}( K={})'.format(self.__class__.__name__,self.K)\n",
    "\n",
    "class LGCN(torch.nn.Module):\n",
    "    def __init__(self,input_size,output_size,hidden_size=512,K=8):\n",
    "        super(LGCN,self).__init__()\n",
    "        self.conv1 = CombUnweighted(K=K)\n",
    "        self.linear = torch.nn.Linear(input_size * (K+1),output_size)\n",
    "    \n",
    "    def forward(self,feature,edge_index):\n",
    "        x = self.conv1(feature,edge_index)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "class notrans(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(notrans, self).__init__()\n",
    "    def forward(self, input_embd):\n",
    "        return input_embd\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "8d528e9f-743f-47e6-af0c-367960ac9c30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGCN(\n",
       "  (conv1): CombUnweighted( K=8)\n",
       "  (linear): Linear(in_features=4842, out_features=512, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "networks = []\n",
    "features = [c.cpu(), d.cpu()]\n",
    "feature_size = c.size(1)\n",
    "feature_output_size = 512\n",
    "torch.seed()\n",
    "model = LGCN(feature_size, 512)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "62a16c02-d217-4957-aebd-582362dc21df",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(LGCN(\n",
      "  (conv1): CombUnweighted( K=8)\n",
      "  (linear): Linear(in_features=4842, out_features=512, bias=True)\n",
      "), None, tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([[   0,    0,    1,  ..., 3903, 3904, 3905],\n",
      "        [   1,    2,    0,  ..., 1900,  187,   72]])), (LGCN(\n",
      "  (conv1): CombUnweighted( K=8)\n",
      "  (linear): Linear(in_features=4842, out_features=512, bias=True)\n",
      "), None, tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([[   0,    1,    1,  ..., 1115, 1116, 1117],\n",
      "        [   1,    0,   84,  ...,  105,  866,  105]])), (LGCN(\n",
      "  (conv1): CombUnweighted( K=8)\n",
      "  (linear): Linear(in_features=4842, out_features=512, bias=True)\n",
      "), None, tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([[   0,    0,    1,  ..., 3903, 3904, 3905],\n",
      "        [   1,    2,    0,  ..., 1900,  187,   72]])), (LGCN(\n",
      "  (conv1): CombUnweighted( K=8)\n",
      "  (linear): Linear(in_features=4842, out_features=512, bias=True)\n",
      "), None, tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([[   0,    1,    1,  ..., 1115, 1116, 1117],\n",
      "        [   1,    0,   84,  ...,  105,  866,  105]])), (LGCN(\n",
      "  (conv1): CombUnweighted( K=8)\n",
      "  (linear): Linear(in_features=4842, out_features=512, bias=True)\n",
      "), None, tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([[   0,    0,    1,  ..., 3903, 3904, 3905],\n",
      "        [   1,    2,    0,  ..., 1900,  187,   72]])), (LGCN(\n",
      "  (conv1): CombUnweighted( K=8)\n",
      "  (linear): Linear(in_features=4842, out_features=512, bias=True)\n",
      "), None, tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([[   0,    1,    1,  ..., 1115, 1116, 1117],\n",
      "        [   1,    0,   84,  ...,  105,  866,  105]]))]\n"
     ]
    }
   ],
   "source": [
    "#features[0].shape\n",
    "edge_1 = torch.LongTensor(np.array(e1.nonzero()))\n",
    "edge_2 = torch.LongTensor(np.array(e2.nonzero()))\n",
    "edge = [edge_1,edge_2]\n",
    "edges = []\n",
    "edges.append(edge_1.cpu())\n",
    "edges.append(edge_2.cpu())\n",
    "edges[0]\n",
    "\n",
    "optimizer = None\n",
    "for i in range(2):\n",
    "    networks.append((model, optimizer, features[i],ed[i]))\n",
    "\n",
    "print(networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "56a5a640-2a60-42a9-b511-df60fb51f7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = notrans()\n",
    "optimizer_trans = torch.optim.Adam(itertools.chain(trans.parameters(),networks[0][0].parameters()),lr=1e-4,weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "cd58ff3f-e4cc-45c0-b1be-9c770bf01a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(indices=tensor([[   0,    0,    0,  ..., 3904, 3905, 3905],\n",
      "                       [   0,    1,    2,  ..., 3904,   72, 3905]]),\n",
      "       values=tensor([0.3333, 0.2357, 0.1021,  ..., 0.5000, 0.2357, 0.5000]),\n",
      "       device='cuda:0', size=(3906, 3906), nnz=20234, layout=torch.sparse_coo)\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "2d608802-454c-411d-a3a6-5403a875da5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3906, 512]), torch.Size([1118, 512]))"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emd0 = networks[0][0](features[0],edges[0])\n",
    "emd1 = networks[1][0](features[1],edges[1])\n",
    "emd0.shape,emd1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "0049ba08-08de-479c-8ca6-2950308f68a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class WDiscriminator(torch.nn.Module):\n",
    "    def __init__(self, hidden_size, hidden_size2=512):\n",
    "        super(WDiscriminator, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(hidden_size, hidden_size2)\n",
    "        self.hidden2 = torch.nn.Linear(hidden_size2, hidden_size2)\n",
    "        self.output = torch.nn.Linear(hidden_size2, 1)\n",
    "    def forward(self, input_embd):\n",
    "        return self.output(F.leaky_relu(self.hidden2(F.leaky_relu(self.hidden(input_embd), 0.2, inplace=True)), 0.2, inplace=True))\n",
    "\n",
    "def pairwise_loss(x1ns, y1ns):\n",
    "    # f(x_1, y_1, x_1_neighbors, y_1_neighbors)\n",
    "    # x_1 has anchor link with y1\n",
    "    # x_2 is neighbor with x_1, y_2 is the neighbor of y_1\n",
    "    # Push up maximum similarity between pairs of x1ns and y1ns\n",
    "    l1 = x1ns.size(0)\n",
    "    l2 = y1ns.size(0)\n",
    "    x1ns = x1ns.unsqueeze(1).expand(-1, l2, -1)\n",
    "    y1ns = y1ns.unsqueeze(0).expand(l1, -1, -1)\n",
    "    cos_sim = F.cosine_similarity(x1ns, y1ns, dim=-1)\n",
    "    loss = - cos_sim.mean()\n",
    "    return loss\n",
    "    \n",
    "def feature_reconstruct_loss(embd, x, recon_model):\n",
    "    recon_x = recon_model(embd)\n",
    "    return torch.norm(recon_x - x, dim=1, p=2).mean()\n",
    "\n",
    "class ReconDNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_size, feature_size, hidden_size2=512):\n",
    "        super(ReconDNN, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(hidden_size, hidden_size2)\n",
    "        self.output = torch.nn.Linear(hidden_size2, feature_size)\n",
    "    def forward(self, input_embd):\n",
    "        return self.output(F.relu(self.hidden(input_embd)))\n",
    "\n",
    "def train_feature_recon(trans, optimizer_trans, networks, recon_models, optimizer_recons, batch_r_per_iter=10):\n",
    "    models = [t[0] for t in networks]\n",
    "    features = [t[2] for t in networks]\n",
    "    edges = [t[3] for t in networks]\n",
    "    recon_model0, recon_model1 = recon_models\n",
    "    optimizer_recon0, optimizer_recon1 = optimizer_recons\n",
    "    embd0 = models[0](features[0], edges[0])\n",
    "    embd1 = trans(models[1](features[1], edges[1]))\n",
    "    recon_model0.train()\n",
    "    recon_model1.train()\n",
    "    trans.train()\n",
    "    models[0].train()\n",
    "    models[1].train()\n",
    "    embd0_copy = embd0.clone().detach()\n",
    "    embd1_copy = embd1.clone().detach()\t\n",
    "    for t in range(batch_r_per_iter):\n",
    "        optimizer_recon0.zero_grad()\n",
    "        loss = feature_reconstruct_loss(embd0_copy, features[0], recon_model0)\n",
    "        loss.backward()\n",
    "        optimizer_recon0.step()\n",
    "    for t in range(batch_r_per_iter):\n",
    "        optimizer_recon1.zero_grad()\n",
    "        loss = feature_reconstruct_loss(embd1_copy, features[1], recon_model1)\n",
    "        loss.backward()\n",
    "        optimizer_recon1.step()\n",
    "    loss = 0.5 * feature_reconstruct_loss(embd0, features[0], recon_model0) + 0.5 * feature_reconstruct_loss(embd1, features[1], recon_model1)\n",
    "\n",
    "    return loss\n",
    "def train_wgan_adv_pseudo_self( trans, optimizer_trans, wdiscriminator, optimizer_d, networks, lambda_gp=10, batch_d_per_iter=5, batch_size_align=512):\n",
    "\t\"\"\"\n",
    "\t\tLGCN + Wasserstein\n",
    "\t:param trans: 要么是T 要么不是\n",
    "\t:param optimizer_trans: 优化器\n",
    "\t:param wdiscriminator: Wasserstein distance\n",
    "\t:param optimizer_d:\n",
    "\t:param networks: two\n",
    "\t:param lambda_gp:\n",
    "\t:param batch_d_per_iter:\n",
    "\t:param batch_size_align:\n",
    "\t:return:\n",
    "\t\"\"\"\n",
    "\t\" 解包，获得两个对应 \"\n",
    "\tmodels = [t[0] for t in networks]\n",
    "\tfeatures = [t[2] for t in networks]\n",
    "\tedges = [t[3] for t in networks]\n",
    "\n",
    "\t\" 1直接送进LGCN计算embedding \"\n",
    "\tembd0 = models[0](features[0], edges[0]) #Tensor:(3906, 512)\n",
    "\t\" 2先LGCN Embedding后在进行T相乘；思想就是paper里面的 \"\n",
    "\tembd1 = trans(models[1](features[1], edges[1])) #Tensor:(1118, 512)\n",
    "\n",
    "\n",
    "\ttrans.train()\n",
    "\twdiscriminator.train()\n",
    "\tmodels[0].train()\n",
    "\tmodels[1].train()\n",
    "\n",
    "\n",
    "\t\"\"\"\n",
    "\t\t迭代次数：5\n",
    "\t\"\"\"\n",
    "\tfor j in range(batch_d_per_iter):\n",
    "\t\t\" 计算两个Wasserstein distance \"\n",
    "\t\tw0 = wdiscriminator(embd0) #Tensor:(3906, 1)\n",
    "\t\tw1 = wdiscriminator(embd1) #Tensor:(1118, 1)\n",
    "\n",
    "\t\t\" 选取最小的前size(0)  注意 这是以节点下标的形式给出\"\n",
    "\t\t\" descending 是降序 \"\n",
    "\t\tanchor1 = w1.view(-1).argsort(descending=True)[: embd1.size(0)] #Tensor:(1118,)\n",
    "\t\tanchor0 = w0.view(-1).argsort(descending=False)[: embd1.size(0)] #Tensor:(1118,)\n",
    "\n",
    "\t\t\"\"\"\n",
    "\t\t\t需要走注意的是，通过.detach() “分离”得到的的变量会和原来的变量共用同样的数据，\n",
    "\t\t\t而且新分离得到的张量是不可求导的\n",
    "\t\t\t\n",
    "\t\t\t底下相当于取得是1118行的？\n",
    "\t\t\"\"\"\n",
    "\t\tembd0_anchor = embd0[anchor0, :].clone().detach() #Tensor(1118, 512)\n",
    "\t\tembd1_anchor = embd1[anchor1, :].clone().detach() #Tensor(1118, 512)\n",
    "\t\toptimizer_d.zero_grad() #梯度置零\n",
    "\t\tloss = -torch.mean(wdiscriminator(embd0_anchor)) + torch.mean(wdiscriminator(embd1_anchor)) #计算损失\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer_d.step() #梯度优化\n",
    "\t\tfor p in wdiscriminator.parameters():\n",
    "\t\t\tp.data.clamp_(-0.1, 0.1)\n",
    "\n",
    "\tw0 = wdiscriminator(embd0)\n",
    "\tw1 = wdiscriminator(embd1)\n",
    "\tanchor1 = w1.view(-1).argsort(descending=True)[: embd1.size(0)]\n",
    "\tanchor0 = w0.view(-1).argsort(descending=False)[: embd1.size(0)]\n",
    "\tembd0_anchor = embd0[anchor0, :]\n",
    "\tembd1_anchor = embd1[anchor1, :]\n",
    "\tloss = -torch.mean(wdiscriminator(embd1_anchor))\n",
    "\treturn loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "6f559363-eaba-413a-a847-3ff76363dd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wdiscriminator = WDiscriminator(feature_output_size)\n",
    "optimizer_wd = torch.optim.Adam(wdiscriminator.parameters(),lr = 0.01, weight_decay=5e-4)\n",
    "recon_model0 = ReconDNN(feature_output_size,feature_size)\n",
    "recon_model1 = ReconDNN(feature_output_size,feature_size)\n",
    "optimizer_recon0 = torch.optim.Adam(recon_model0.parameters(),lr=0.01,weight_decay=5e-4)\n",
    "optimizer_recon1 = torch.optim.Adam(recon_model1.parameters(),lr=0.01,weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "dce21920-6897-46d0-a97e-5c6c4de7109b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 579, 2879, 1142,  ...,  309, 1552, 1376],\n",
       "        [ 725,  661, 1010,  ...,  674,  868,  665]], dtype=torch.int32)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groundtruth = load_gt(\"graph_data/douban/dictionaries/groundtruth\", source_dataset.id2idx, target_dataset.id2idx, 'dict')\n",
    "type(groundtruth)\n",
    "g = []\n",
    "g.append(list(groundtruth.keys()))\n",
    "g.append(list(groundtruth.values()))\n",
    "\n",
    "ground_truth = torch.from_numpy(np.array(g))\n",
    "ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "54debddd-be53-48ce-86af-7e2ff0795f7a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_align(embds, ground_truth, k=5, mode='cosine', prior=None, prior_rate=0):\n",
    "\tembd0, embd1 = embds  #list解包操作。将原来的两个tensor矩阵分别给到两个嵌入\n",
    "\tg_map = {}\n",
    "\t\"\"\"\n",
    "\t\t\n",
    "\t\tground_truth为：tensor([[2890, 3866, 1883,  ...,  583, 3904, 2746],[ 902,  270,  889,  ...,  275,  915,  801]], dtype=torch.int32)\n",
    "\t\tground_truth.size() 为 torch.Size([2, 1118])  torch.Size括号中有几个数字就是几维\n",
    "\t\tground_truth.size(1) 为1118 代表的是ground_truth的第二个索引的值。\n",
    "\t\"\"\"\n",
    "\tfor i in range(ground_truth.size(1)):\n",
    "\t\t\"\"\"\n",
    "\t\t\tground_truth[0, i].item()为：2890  3866 1883 等\n",
    "\t\t\t把这些加入到dict中，对齐的字典\n",
    "\t\t\tg_map最后变成：{902: 2890, 270: 3866, 889: 1883, ...... }\n",
    "\t\t\"\"\"\n",
    "\t\tg_map[ground_truth[1, i].item()] = ground_truth[0, i].item()\n",
    "\tg_list = list(g_map.keys()) # 将字典里面的key转成一个单独的list\n",
    "\t\n",
    "\tcossim = torch.zeros(embd1.size(0), embd0.size(0)) #构造一个全是0的tensor，torch.Size([1118, 3906])\n",
    "\tfor i in range(embd1.size(0)):\n",
    "\t\t\"\"\"\n",
    "\t\t\tF.cosine_similarity是计算余弦相似度的函数。\n",
    "\t\t\"\"\"\n",
    "\t\tcossim[i] = F.cosine_similarity(embd0, embd1[i:i+1].expand(embd0.size(0), embd1.size(1)), dim=-1).view(-1)\n",
    "\tif prior is not None:\n",
    "\t\tcossim = (1 + cossim)/2 * (1-prior_rate) + prior * prior_rate\n",
    "\n",
    "\t\"\"\"\n",
    "\t\tk是多少 就取多少列，行数代表图1中的节点个数\n",
    "\t\t\n",
    "\t\ttensor([[2952, 1252, 3707, 2453, 1598],\n",
    "        [3602, 3417, 1479, 2268, 2450],\n",
    "        [2013,  670, 3047, 3169, 2549],\n",
    "        ...,\n",
    "        [1010,  606, 3865, 3203, 2111],\n",
    "        [3321, 1728, 1056, 2806,  301],\n",
    "        [ 979, 2539,  318, 1764, 2205]])\n",
    "\t\"\"\"\n",
    "\tind = cossim.argsort(dim=1, descending=True)[:, :k] #Tensor:(1118, 5)\n",
    "\ta1 = 0\n",
    "\tak = 0 \n",
    "\tfor i, node in enumerate(g_list): #以枚举形式给出，i代表苏音号，从0 -> len(g_list) ; node 为\n",
    "\t\t\"\"\"\n",
    "\t\t\t计算关键指标的方法：\n",
    "\t\t\t\t如果\n",
    "\t\t\"\"\"\n",
    "\t\tif ind[node, 0].item() == g_map[node]:\n",
    "\t\t\ta1 += 1\n",
    "\t\t\tak += 1\n",
    "\t\telse:\n",
    "\t\t\tfor j in range(1, ind.shape[1]):\n",
    "\t\t\t\tif ind[node, j].item() == g_map[node]:\n",
    "\t\t\t\t\tak += 1\n",
    "\t\t\t\t\tbreak\n",
    "\ta1 /= len(g_list)\n",
    "\tak /= len(g_list)\n",
    "\tprint('H@1 %.2f%% H@5 %.2f%%' % (a1*100, ak*100))\n",
    "\treturn a1, ak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "0d63ce16-e81d-4789-9604-865ba3f9aef1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H@1 20.57% H@5 36.58%\n",
      "H@1 20.30% H@5 36.49%\n",
      "H@1 20.48% H@5 36.58%\n",
      "H@1 20.39% H@5 36.40%\n",
      "H@1 20.39% H@5 36.31%\n",
      "Total time 45.55\n",
      "H@1 20.57% H@5 36.58%\n"
     ]
    }
   ],
   "source": [
    "batch_size_align = 128\n",
    "best = 0\n",
    "bp=0,0\n",
    "import time\n",
    "time1 = time.time()\n",
    "for i in range(1, 5 +1):\n",
    "    trans.train()\n",
    "    networks[0][0].train()\n",
    "    networks[1][0].train()\n",
    "    loss = train_wgan_adv_pseudo_self(trans, optimizer_trans, wdiscriminator, optimizer_wd, networks)\n",
    "\n",
    "    # 损失函数计算， 其中alpha是超参数\n",
    "    loss_feature = train_feature_recon(trans, optimizer_trans, networks, [recon_model0, recon_model1], [optimizer_recon0, optimizer_recon1])\n",
    "    loss = (1-0.01) * loss + 0.01 * loss_feature\n",
    "\n",
    "    #反向传播\n",
    "    loss.backward()\n",
    "    #把梯度进行更新\n",
    "    optimizer_trans.step()\n",
    "    \n",
    "    networks[0][0].eval()\n",
    "    networks[1][0].eval()\n",
    "    trans.eval()\n",
    "    embd0 = networks[0][0](features[0], edges[0])\n",
    "    embd1 = networks[1][0](features[1], edges[1])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        a1, ak = check_align([embd0, trans(embd1)], ground_truth, mode='cosine', prior=None, prior_rate=0)\n",
    "    if a1 > best:\n",
    "        best = a1\n",
    "        bp = a1, ak\n",
    "\n",
    "time2 = time.time()\n",
    "print(\"Total time %.2f\" % ((time2-time1)))\n",
    "print(\"H@1 %.2f%% H@5 %.2f%%\" % (bp[0]*100,bp[1]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "b1564b45-25dd-4a30-96cf-9e86e73254db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "time1 = time.time()\n",
    "time2 = time.time()\n",
    "time2 - time1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a500d723-9654-4fa1-9ce0-a571e401cb56",
   "metadata": {},
   "source": [
    "# 留一个问题 之所以上升这么慢 是因为学习率的问题？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca273bff-92d8-4f18-8fc8-74da0e027c18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
